{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Matrix(*a):\n",
    "    if len(a)==1 and isinstance(a[0], np.ndarray):\n",
    "        a = a[0]\n",
    "    return np.array([[float(x) for x in r] for r in a])\n",
    "\n",
    "def Vector(*a):\n",
    "    if len(a)==1 and isinstance(a[0], np.ndarray):\n",
    "        a = a[0]\n",
    "    return np.array([float(x) for x in a]).reshape(-1,1)\n",
    "\n",
    "# Black magic\n",
    "from IPython.display import Latex, SVG, display\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def ndarray_to_latex(arr): \n",
    "    if len(arr.shape)==1: \n",
    "        arr=arr.reshape(1,-1)\n",
    "    if len(arr.shape) == 2:\n",
    "        if max(arr.shape) > 30:\n",
    "            return None\n",
    "        str_arr = np.vectorize(\"{:.3f}\".format)(arr)\n",
    "        return r'\\begin{{pmatrix}}{}\\end{{pmatrix}}'.format(r'\\\\ '.join(map('&'.join, str_arr))) \n",
    "    if len(arr.shape) == 3 and arr.shape[2]==1:\n",
    "        if max(arr.shape) > 30:\n",
    "            return None\n",
    "        arr = arr[:,:,0]\n",
    "        str_arr = np.vectorize(\"{:.3f}\".format)(arr)\n",
    "        return r'\\begin{{bmatrix}}[{}]\\end{{bmatrix}}'.format(\n",
    "            r']\\\\ ['.join(map('&'.join, str_arr))) \n",
    "    return None\n",
    "sh = InteractiveShell.instance()\n",
    "sh.display_formatter.formatters['text/latex'].type_printers[np.ndarray]=ndarray_to_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Network\n",
    "一樣有輸入 x, 輸出  y。 但是中間預測、計算的樣子有點不同。\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/5/54/Feed_forward_neural_net.gif\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型是這樣的\n",
    "一樣考慮輸入是四維向量，輸出有 3 個類別。\n",
    "\n",
    "我們的輸入 $x=\\begin{pmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} $ 是一個向量，我們看成 column vector 好了\n",
    "\n",
    "### 第 0 層\n",
    "而 Weight: $ \n",
    "W^{(0)} = \\begin{pmatrix} W^{(0)}_0 \\\\ W^{(0)}_1 \\\\ W^{(0)}_2 \\\\ W^{(0)}_3 \\\\ W^{(0)}_4 \\\\ W^{(0)}_5 \\end{pmatrix} =  \n",
    "\\begin{pmatrix} \n",
    "W^{(0)}_{0,0} & W^{(0)}_{0,1} &  W^{(0)}_{0,2} & W^{(0)}_{0,3}\\\\ \n",
    "W^{(0)}_{0,0} & W^{(0)}_{0,1} &  W^{(0)}_{0,2} & W^{(0)}_{0,3}\\\\ \n",
    "W^{(0)}_{0,0} & W^{(0)}_{0,1} &  W^{(0)}_{0,2} & W^{(0)}_{0,3}\\\\ \n",
    "W^{(0)}_{0,0} & W^{(0)}_{0,1} &  W^{(0)}_{0,2} & W^{(0)}_{0,3}\\\\ \n",
    "W^{(0)}_{0,0} & W^{(0)}_{0,1} &  W^{(0)}_{0,2} & W^{(0)}_{0,3}\\\\ \n",
    "W^{(0)}_{0,0} & W^{(0)}_{0,1} &  W^{(0)}_{0,2} & W^{(0)}_{0,3}\n",
    " \\end{pmatrix} $\n",
    " \n",
    " Bias: $b^{(0)}=\\begin{pmatrix} b^{(0)}_0 \\\\ b^{(0)}_1 \\\\ b^{(0)}_2  \\\\ b^{(0)}_3 \\\\ b^{(0)}_4 \\\\ b^{(0)}_5 \\end{pmatrix} $ \n",
    "\n",
    "\n",
    "我們先計算\"線性輸出\"  $ c^{(0)} = \\begin{pmatrix} c^{(0)}_0 \\\\ c^{(0)}_1 \\\\ c^{(0)}_2  \\\\ c^{(0)}_3 \\\\ c^{(0)}_4 \\\\ c^{(0)}_5 \\end{pmatrix} =  W^{(0)}x+b^{(0)} =\n",
    "\\begin{pmatrix} W^{(0)}_0 x + b^{(0)}_0 \\\\ W^{(0)}_1 x + b^{(0)}_1 \\\\ W^{(0)}_2 x + b^{(0)}_2 \\\\\n",
    "W^{(0)}_3 x + b^{(0)}_3 \\\\ W^{(0)}_4 x + b^{(0)}_4 \\\\ W^{(0)}_5 x + b^{(0)}_5  \\end{pmatrix}   $， \n",
    "\n",
    "然後再將結果逐項對一個非線性的函數 $f$  最後得到一個向量。\n",
    " \n",
    " $d^{(0)} = \\begin{pmatrix} d^{(0)}_0 \\\\ d^{(0)}_1 \\\\ d^{(0)}_2  \\\\ d^{(0)}_3 \\\\ d^{(0)}_4 \\\\ d^{(0)}_5 \\end{pmatrix} \n",
    " = f({W x + b}) = \\begin{pmatrix} f(c^{(0)}_0) \\\\ f(c^{(0)}_1) \\\\ f(c^{(0)}_2)  \\\\ f(c^{(0)}_3) \\\\ f(c^{(0)}_4) \\\\ f(c^{(0)}_5) \\end{pmatrix} $\n",
    " \n",
    "這裡的 $f$ 常常會用 sigmoid , tanh，或者 ReLU ( https://en.wikipedia.org/wiki/Activation_function )。\n",
    "\n",
    "### 第 1 層\n",
    "這裡接到輸出，其實和 softmax regression 一樣。\n",
    "\n",
    "只是輸入變成 $d^{(0)}$, Weight 和 Bias 現在叫做 $W^{(1)}$ 和 $b^{(1)}$ \n",
    "\n",
    "因為維度改變，現在 $W^{(1)}$ 是 3x6 的矩陣。 後面接到的輸出都一樣。\n",
    "\n",
    "所以線性輸出\n",
    "\n",
    "### $ c^{(1)} =  W^{(1)} d^{(0)} + b^{(1)} $\n",
    "\n",
    "### $ d^{(1)} =  e^{c^{(1)}} $\n",
    "\n",
    "\n",
    "\n",
    "當輸入為 x, 最後的 softmax 預測類別是 i 的機率為\n",
    "###  $q_i = Predict_{W^{(0)}, W^{(1)}, b^{(0)}, b^{(1)}}(Y=i|x)  = \\frac {d^{(1)}_i} {\\sum_j d^{(1)}_j}$\n",
    "### 合起來看，就是 $q = \\frac {d^{(1)}} {\\sum_j d^{(1)}_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題\n",
    "如果 $W^{(0)}, W^{(1)}, b^{(0)}, b^{(1)}$ 設定為 $A, C, b, d$ (C, d 與前面無關)\n",
    "\n",
    "softmax function 用\n",
    "### $\\sigma (\\mathbf {z} )_{j}={\\frac {e^{z_{j}}}{\\sum _k e^{z_{k}}}}$\n",
    "表示，我們簡化上面的過程成為一個算式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "$d^{(0)} = f(W^{(0)}x+b^{(0)})$  will become:  $d^{(0)} = f(Ax+b)$\n",
    "\n",
    "$d^{(1)}=f(W^{(1)}d^{(0)}+b^{(1)})$  will become:  $d^{(1)}=f(Cf(Ax+b)+d)$\n",
    "\n",
    "$\\sigma(W^{(1)}f(W^{(0)}x+b^{(0)})+b^{(1)})$ will become: $\\sigma( Cf(Ax+b)+d )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\sigma (Cf(Ax+b)+d)$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Latex\n",
    "display(Latex(\"$\\sigma (Cf(Ax+b)+d)$\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任務：計算最後的猜測機率 $q$\n",
    "設定：輸入 4 維， 輸出 3 維， 隱藏層 6 維\n",
    "* 設定一些權重 $A,b,C,d$ (隨意自行填入，或者用 np.random.randint(-2,3, size=...))\n",
    "* 設定輸入 $x$ (隨意自行填入，或者用 np.random.randint(-2,3, size=...))\n",
    "* 自行定義 relu, sigmoid 函數 (Hint: np.maximum)\n",
    "* 算出隱藏層 $z$\n",
    "* 自行定義 softmax\n",
    "* 算出最後的 q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "#### Note\n",
    "x is a 4x1 matrix\n",
    "\n",
    "$A = W^{(0)}$ is a 6x4 matrix\n",
    "\n",
    "$b = b^{(0)}$ is a 6x1 matrix\n",
    "\n",
    "$C = W^{(1)}$ is a 3x6 matrix\n",
    "\n",
    "$d = b^{(1)}$ is a 3x1 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 請在這裡計算\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測定權重 A, b, C, d 和 x\n",
    "x = np.random.randint(-2, 3, size=(4, 1))\n",
    "A = np.random.randint(-2, 3, size=(6, 4))\n",
    "b = np.random.randint(-2, 3, size=(6, 1))\n",
    "C = np.random.randint(-2, 3, size=(3, 6))\n",
    "d = np.random.randint(-2, 3, size=(3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}-1.000&-1.000&-1.000&0.000\\\\ 1.000&2.000&2.000&0.000\\\\ 0.000&-2.000&-2.000&2.000\\\\ -2.000&-1.000&0.000&-2.000\\\\ 1.000&2.000&0.000&0.000\\\\ 1.000&1.000&-2.000&-1.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[-1, -1, -1,  0],\n",
       "       [ 1,  2,  2,  0],\n",
       "       [ 0, -2, -2,  2],\n",
       "       [-2, -1,  0, -2],\n",
       "       [ 1,  2,  0,  0],\n",
       "       [ 1,  1, -2, -1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.000\\\\ -2.000\\\\ 1.000\\\\ 0.000\\\\ 1.000\\\\ 2.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 1],\n",
       "       [-2],\n",
       "       [ 1],\n",
       "       [ 0],\n",
       "       [ 1],\n",
       "       [ 2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}-1.000&1.000&1.000&1.000&0.000&-1.000\\\\ 1.000&2.000&0.000&1.000&2.000&-1.000\\\\ 2.000&-2.000&2.000&-1.000&-1.000&2.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[-1,  1,  1,  1,  0, -1],\n",
       "       [ 1,  2,  0,  1,  2, -1],\n",
       "       [ 2, -2,  2, -1, -1,  2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}-1.000\\\\ 2.000\\\\ -2.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[-1],\n",
       "       [ 2],\n",
       "       [-2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.000\\\\ 2.000\\\\ 2.000\\\\ -2.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 1],\n",
       "       [ 2],\n",
       "       [ 2],\n",
       "       [-2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 參考答案，設定權重\n",
    "display(A)\n",
    "display(b)\n",
    "display(C)\n",
    "display(d)\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activation function\n",
    "#### ReLU\n",
    "$f(x)={\\begin{cases}0&{\\text{for }}x\\leq 0\\\\x&{\\text{for }}x>0\\end{cases}}$\n",
    "\n",
    "#### Sigmoid\n",
    "$f(x)=\\sigma (x)={\\frac {1}{1+e^{-x}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 relu, sigmoid 函數\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_relu = relu(A@x + b)\n",
    "d_sigmoid = sigmoid(A@x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.000\\\\ 7.000\\\\ 0.000\\\\ 0.000\\\\ 6.000\\\\ 3.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[0],\n",
       "       [7],\n",
       "       [0],\n",
       "       [0],\n",
       "       [6],\n",
       "       [3]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.018\\\\ 0.999\\\\ 0.000\\\\ 0.500\\\\ 0.998\\\\ 0.953\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[1.79862100e-02],\n",
       "       [9.99088949e-01],\n",
       "       [1.67014218e-05],\n",
       "       [5.00000000e-01],\n",
       "       [9.97527377e-01],\n",
       "       [9.52574127e-01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 參考答案 定義 relu, sigmoid 及計算 z\n",
    "display(d_relu)\n",
    "display(d_sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "$q={\\frac {d^{(1)}}{\\sum _j d^{(1)}_{j}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 softmax 及計算 q\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_relu = softmax(C@relu(A@x+b)+d)\n",
    "q_sigmoid = softmax(C@sigmoid(A@x+b)+d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.000\\\\ 1.000\\\\ 0.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[2.78946809e-10],\n",
       "       [1.00000000e+00],\n",
       "       [1.56288219e-18]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.002\\\\ 0.997\\\\ 0.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[2.39921970e-03],\n",
       "       [9.97490855e-01],\n",
       "       [1.09925441e-04]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 參考答案 定義 softmax 及計算 q\n",
    "display(q_relu)\n",
    "display(q_sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習\n",
    "設計一個網路:\n",
    "* 輸入是二進位 0 ~ 15\n",
    "* 輸出依照對於 3 的餘數分成三類\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.000\\\\ 0.000\\\\ 1.000\\\\ 1.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hint 下面產生數字 i 的 2 進位向量\n",
    "i = 13\n",
    "x = Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0, i%3=0, q=0\n",
      "i=1, i%3=1, q=1\n",
      "i=2, i%3=2, q=2\n",
      "i=3, i%3=0, q=0\n",
      "i=4, i%3=1, q=1\n",
      "i=5, i%3=2, q=2\n",
      "i=6, i%3=0, q=0\n",
      "i=7, i%3=1, q=1\n",
      "i=8, i%3=2, q=2\n",
      "i=9, i%3=0, q=0\n",
      "i=10, i%3=1, q=1\n",
      "i=11, i%3=2, q=2\n",
      "i=12, i%3=0, q=0\n",
      "i=13, i%3=1, q=1\n",
      "i=14, i%3=2, q=2\n",
      "i=15, i%3=0, q=0\n"
     ]
    }
   ],
   "source": [
    "# 請在這裡計算\n",
    "# mod 3\n",
    "A = Matrix([0,0,0,0], \n",
    "           [1,-1,1,-1], \n",
    "           [-1,1,-1,1],\n",
    "           [-10,10,-10,10],\n",
    "           [10,-10,10,-10],\n",
    "          )\n",
    "b = Vector(0.1,0,0,-12,-12)\n",
    "C = Matrix([1,0,0,0,0], \n",
    "           [0,1,0,1,0], \n",
    "           [0,0,1,0,1]\n",
    "          )\n",
    "d = Vector(0,0,0)\n",
    "for i in range(16):\n",
    "    x = Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2)\n",
    "    q = softmax(C@relu(A@x+b)+d)\n",
    "    print(\"i={}, i%3={}, q={}\".format(i, i%3, q.argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0, i%3=0, q=0\n",
      "i=1, i%3=1, q=1\n",
      "i=2, i%3=2, q=2\n",
      "i=3, i%3=0, q=0\n",
      "i=4, i%3=1, q=1\n",
      "i=5, i%3=2, q=1\n",
      "i=6, i%3=0, q=0\n",
      "i=7, i%3=1, q=1\n",
      "i=8, i%3=2, q=2\n",
      "i=9, i%3=0, q=0\n",
      "i=10, i%3=1, q=2\n",
      "i=11, i%3=2, q=2\n",
      "i=12, i%3=0, q=0\n",
      "i=13, i%3=1, q=1\n",
      "i=14, i%3=2, q=2\n",
      "i=15, i%3=0, q=0\n"
     ]
    }
   ],
   "source": [
    "# 請在這裡計算\n",
    "# mod 3 窮舉法XD\n",
    "A = Matrix([0,0,0,0], \n",
    "           [1,1,0,0],\n",
    "           [0,1,1,0],\n",
    "           [0,0,1,1],\n",
    "           [1,0,0,1],\n",
    "           [1,1,1,1],\n",
    "           [1,0,0,0],\n",
    "           [0,0,1,0],\n",
    "           [0,1,0,1],\n",
    "           [1,1,1,0],\n",
    "           [1,0,1,1],\n",
    "           [0,1,0,0],\n",
    "           [0,0,0,1],\n",
    "           [1,0,1,0],\n",
    "           [1,1,0,1],\n",
    "           [0,1,1,1]\n",
    "          )\n",
    "b = Vector(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\n",
    "C = Matrix([1/6,1/6,1/6,1/6,1/6,1/6,0,0,0,0,0,0,0,0,0,0], \n",
    "           [0,0,0,0,0,0,1/5,1/5,1/5,1/5,1/5,0,0,0,0,0], \n",
    "           [0,0,0,0,0,0,0,0,0,0,0,1/5,1/5,1/5,1/5,1/5]\n",
    "          )\n",
    "d = Vector(0,0,0)\n",
    "for i in range(16):\n",
    "    x = Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2)\n",
    "    q = softmax(C@relu(A@x+b)+d)\n",
    "    print(\"i={}, i%3={}, q={}\".format(i, i%3, q.argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0, i%4=0, q=0\n",
      "i=1, i%4=1, q=1\n",
      "i=2, i%4=2, q=2\n",
      "i=3, i%4=3, q=3\n",
      "i=4, i%4=0, q=0\n",
      "i=5, i%4=1, q=1\n",
      "i=6, i%4=2, q=2\n",
      "i=7, i%4=3, q=3\n",
      "i=8, i%4=0, q=0\n",
      "i=9, i%4=1, q=1\n",
      "i=10, i%4=2, q=2\n",
      "i=11, i%4=3, q=3\n",
      "i=12, i%4=0, q=0\n",
      "i=13, i%4=1, q=1\n",
      "i=14, i%4=2, q=2\n",
      "i=15, i%4=3, q=3\n"
     ]
    }
   ],
   "source": [
    "# 請在這裡計算\n",
    "# mod 4\n",
    "A = Matrix([-1,-1,0,0],\n",
    "           [1,-1,0,0],\n",
    "           [-1,1,0,0],\n",
    "           [1,1,0,0]\n",
    "          )\n",
    "b = Vector(1,0,0,0)\n",
    "C = Matrix([1,0,0,0], \n",
    "           [0,1,0,0], \n",
    "           [0,0,1,0],\n",
    "           [0,0,0,1]\n",
    "          )\n",
    "d = Vector(0,0,0,0)\n",
    "for i in range(16):\n",
    "    x = Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2)\n",
    "    q = softmax(C@relu(A@x+b)+d)\n",
    "    print(\"i={}, i%4={}, q={}\".format(i, i%4, q.argmax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習\n",
    "設計一個網路來判斷井字棋是否有連成直線(只需要判斷其中一方即可):\n",
    "* 輸入是 9 維向量，0 代表空格，1 代表有下子 \n",
    "* 輸出是二維(softmax)或一維(sigmoid)皆可，用來代表 True, False\n",
    "\n",
    "有連線的例子\n",
    "\n",
    "```\n",
    "_X_\n",
    "X__\n",
    "XXX\n",
    "\n",
    "XXX\n",
    "XX_\n",
    "_XX\n",
    "\n",
    "__X\n",
    "_XX\n",
    "X__\n",
    "```\n",
    "\n",
    "沒連線的例子\n",
    "```\n",
    "XX_\n",
    "X__\n",
    "_XX\n",
    "\n",
    "_X_\n",
    "XX_\n",
    "X_X\n",
    "\n",
    "__X\n",
    "_XX\n",
    "_X_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 請在這裡計算\n",
    "# 連線的轉置矩陣\n",
    "A = Matrix([1,1,1,0,0,0,0,0,0],\n",
    "           [0,0,0,1,1,1,0,0,0],\n",
    "           [0,0,0,0,0,0,1,1,1],\n",
    "           [1,0,0,1,0,0,1,0,0],\n",
    "           [0,1,0,0,1,0,0,1,0],\n",
    "           [0,0,1,0,0,1,0,0,1],\n",
    "           [0,0,1,0,1,0,1,0,0],\n",
    "           [1,0,0,0,1,0,0,0,1]\n",
    "          )\n",
    "# 只有連線的組合經過Ax後會等於3，因此將bias設成-2是其他沒有連線的組合經過Ax-b後會小於等於零\n",
    "b = Vector(-2,-2,-2,-2,-2,-2,-2,-2)\n",
    "C = Matrix([-1,-1,-1,-1,-1,-1,-1,-1], \n",
    "           [1,1,1,1,1,1,1,1]\n",
    "          )\n",
    "# 為了讓沒有連線的輸出較大\n",
    "d = Vector(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed\n"
     ]
    }
   ],
   "source": [
    "# 測試你的答案\n",
    "def my_result(x):\n",
    "    # return 0 means no, 1 means yes\n",
    "    return (C@relu(A@x+b)+d).argmax()\n",
    "    # or sigmoid based\n",
    "    # return (C@relu(A@x+b)+d) > 0\n",
    "\n",
    "def truth(x):\n",
    "    x = x.reshape(3,3)\n",
    "    return (x.all(axis=0).any() or\n",
    "            x.all(axis=1).any() or\n",
    "            x.diagonal().all() or\n",
    "            x[::-1].diagonal().all())\n",
    "\n",
    "for i in range(512):\n",
    "    x = np.array([[(i>>j)&1] for j in range(9)])\n",
    "    assert my_result(x) == truth(x)\n",
    "print(\"test passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
